{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS515 Project (CONV_KNRM).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca-FV6wcpFY4"
      },
      "source": [
        "# Deep Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWVf9Tgn1pKf"
      },
      "source": [
        "!pip install trectools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rCtuxSSZRnd"
      },
      "source": [
        "# implement this in torch or keras\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "from sklearn.metrics import ndcg_score\n",
        "from trectools import TrecQrel, TrecRun, TrecEval\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import Dot\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "\n",
        "\n",
        "from keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tjzw_gXZLGN",
        "outputId": "8a19ded6-3025-40cf-8346-80d96492d17d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-29 10:21:53--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.94.93\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.94.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  16.6MB/s    in 99s     \n",
            "\n",
            "2020-05-29 10:23:33 (15.9 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4ppXKLlkEbJ",
        "outputId": "83dddd62-6221-4a4c-ad43-2980efb29d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!cp \"drive/My Drive/data_folds.zip\" ./\n",
        "!unzip data_folds.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data_folds.zip\n",
            "   creating: data/\n",
            "  inflating: data/fold_2.train       \n",
            "  inflating: data/fold_2.val         \n",
            "  inflating: data/fold_1.train       \n",
            "  inflating: data/fold_4.val         \n",
            "  inflating: data/fold_1.val         \n",
            "  inflating: data/fold_5.val         \n",
            "  inflating: data/fold_3.test        \n",
            "  inflating: data/fold_3.val         \n",
            "  inflating: data/document_contents.json  \n",
            "  inflating: data/fold_4.train       \n",
            "  inflating: data/fold_3.train       \n",
            "  inflating: data/fold_5.test        \n",
            "  inflating: data/fold_5.train       \n",
            "  inflating: data/fold_1.test        \n",
            "  inflating: data/fold_4.test        \n",
            "  inflating: data/fold_2.test        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGxK4efHlEL2"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('drive/My Drive/document_contents.json') as json_file:\n",
        "    document_contents = json.load(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F0AU5sUqhSk"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Georgetown-IR-Lab/cedr/master/data/robust/queries.tsv\n",
        "with open('queries.tsv','r') as f:\n",
        "  queries = {}\n",
        "  for line in f:\n",
        "    cols = line.rstrip().split('\\t')\n",
        "    c_type, c_id, c_text = cols\n",
        "    queries[c_id] = c_text\n",
        "queries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohcP5ADz0I3Z",
        "outputId": "46f64ed8-2eb5-4a7f-c814-d1fed0287633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget https://trec.nist.gov/data/robust/qrels.robust2004.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-29 14:18:41--  https://trec.nist.gov/data/robust/qrels.robust2004.txt\n",
            "Resolving trec.nist.gov (trec.nist.gov)... 129.6.13.51, 2610:20:6b01:4::36\n",
            "Connecting to trec.nist.gov (trec.nist.gov)|129.6.13.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6543541 (6.2M) [text/plain]\n",
            "Saving to: ‘qrels.robust2004.txt.1’\n",
            "\n",
            "qrels.robust2004.tx 100%[===================>]   6.24M  4.02MB/s    in 1.6s    \n",
            "\n",
            "2020-05-29 14:18:44 (4.02 MB/s) - ‘qrels.robust2004.txt.1’ saved [6543541/6543541]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF-nGRO80Khm"
      },
      "source": [
        "qrels = {}\n",
        "with open('qrels.robust2004.txt','r') as f:\n",
        "  for line in f:\n",
        "    qid, _, docid, score = line.split()\n",
        "    qrels.setdefault(qid, {})[docid] = int(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nrVEmgZkZvM"
      },
      "source": [
        "# try one fold for now\n",
        "fold_name = \"fold_1\"\n",
        "training_file = fold_name+\".train\"\n",
        "test_file = fold_name+\".test\"\n",
        "val_file = fold_name+\".val\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G3vu-E_p_QR"
      },
      "source": [
        "training_docs = []\n",
        "training_doc_ids = []\n",
        "training_query_ids = []\n",
        "training_queries = []\n",
        "training_labels = []\n",
        "errors = set()\n",
        "\n",
        "with open('data/'+training_file,'r') as f:\n",
        "  for line in f:\n",
        "    query = line.split()[0]\n",
        "    doc = line.split()[1]\n",
        "    \n",
        "    try:\n",
        "      qrels[query]\n",
        "      document_contents[doc]\n",
        "      # get label (if in qrels and >0 then label = 1)\n",
        "      label = 0.0\n",
        "      if(doc in qrels[query] and (qrels[query][doc] > 0)):\n",
        "        label = 1.0\n",
        "      training_labels.append(label)\n",
        "\n",
        "      training_doc_ids.append(doc)\n",
        "      training_query_ids.append(query)\n",
        "\n",
        "\n",
        "      training_docs.append(document_contents[doc])\n",
        "      training_queries.append(queries[query])\n",
        "\n",
        "    except:\n",
        "      errors.add(doc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlmiwVKi4nXq",
        "outputId": "f197906f-e23b-4774-d0a0-3341404989ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# manual checking if labels are valid\n",
        "i = 1000\n",
        "print(training_doc_ids[i])\n",
        "print(training_query_ids[i])\n",
        "print(training_labels[i])\n",
        "print(\"real label\")\n",
        "print(qrels[training_query_ids[i]][training_doc_ids[i]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FT933-939\n",
            "331\n",
            "1.0\n",
            "real label\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar9Py3YAsJ1S",
        "outputId": "33932ed5-e39f-4198-c6dd-c03be3889860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_queries[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'International Organized Crime'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUBwg-GezCP2",
        "outputId": "e3255892-a1f3-48e0-c730-22d85d40db13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "max_len  = 0\n",
        "for q in training_docs:\n",
        "  if(max_len < len(q.split())):\n",
        "    max_len = len(q.split())\n",
        "print('longest doc')\n",
        "max_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "longest doc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c05gDk8gtphZ",
        "outputId": "f48ad06b-c3de-41c1-a218-0eed2ce71fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "max_len  = 0\n",
        "for q in training_queries:\n",
        "  if(max_len < len(q.split())):\n",
        "    max_len = len(q.split())\n",
        "print('longest query')\n",
        "max_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "longest query\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm4jw-HMtnL8"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(training_docs)\n",
        "\n",
        "tokenized_docs = tokenizer.texts_to_sequences(training_docs)\n",
        "tokenized_queries = tokenizer.texts_to_sequences(training_queries)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "doc_maxlen = 1024\n",
        "query_max_len = 5\n",
        "\n",
        "X_docs = pad_sequences(tokenized_docs, maxlen=doc_maxlen)\n",
        "X_queries = pad_sequences(tokenized_queries, maxlen=query_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmoX5jaipxYH"
      },
      "source": [
        "test_docs = []\n",
        "test_doc_ids = []\n",
        "test_query_ids = []\n",
        "test_queries = []\n",
        "test_labels = []\n",
        "errors = []\n",
        "with open('data/'+test_file,'r') as f:\n",
        "  for line in f:\n",
        "    try:\n",
        "      query = line.split()[0]\n",
        "      doc = line.split()[1]\n",
        "      document_contents[doc]\n",
        "      queries[query]\n",
        "      qrels[query][doc]\n",
        "\n",
        "\n",
        "\n",
        "      test_doc_ids.append(doc)\n",
        "      test_query_ids.append(query)\n",
        "\n",
        "      \n",
        "      test_docs.append(document_contents[doc])\n",
        "      test_queries.append(queries[query])\n",
        "\n",
        "      # get label (if in qrels and >0 then label = 1)\n",
        "      label = 0\n",
        "      if(doc in qrels[query] and (qrels[query][doc] > 0)):\n",
        "        label = 1\n",
        "      test_labels.append(label)\n",
        "\n",
        "      \n",
        "    except:\n",
        "      errors.append([query,doc])\n",
        "\n",
        "\n",
        "tokenized_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "tokenized_queries = tokenizer.texts_to_sequences(test_queries)\n",
        "\n",
        "X_docs_test = pad_sequences(tokenized_docs, maxlen=doc_maxlen)\n",
        "X_queries_test = pad_sequences(tokenized_queries, maxlen=query_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZznjm8Ryh49",
        "outputId": "e58fe999-65e3-4078-b879-8137c2666c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_docs_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6669"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wZyoBX8ygoL",
        "outputId": "2ee575a9-2919-4ce5-de32-02e35abd6d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6669"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiBvcBlOvsWi",
        "outputId": "52143e3a-7ced-4a06-8ab4-16baa47e251c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "print(\"loading word2vec model…\")\n",
        "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "def getVector(str):\n",
        "  if str in word2vec_model:\n",
        "    return word2vec_model[str]\n",
        "  else:\n",
        "    return None;\n",
        "def isInModel(str):\n",
        "  return str in word2vec_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-29 18:19:04,362 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading word2vec model…\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-05-29 18:21:03,704 : INFO : loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozq5-4UCv4LD"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "EMBEDDING_DIM = 300 \n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  embedding_vector = getVector(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihcXGQ4Swuun",
        "outputId": "e4d2b557-76c2-453b-99f1-87c95cf9d654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(193308, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5YCQGk84_kW"
      },
      "source": [
        "training_labels_dicts = {}\n",
        "for i in range(len(training_query_ids)):\n",
        "  training_labels_dicts.setdefault(training_query_ids[i], {})[training_doc_ids[i]] = training_labels[i]\n",
        "\n",
        "\n",
        "test_labels_dicts = {}\n",
        "for i in range(len(test_query_ids)):\n",
        "  test_labels_dicts.setdefault(test_query_ids[i], {})[test_doc_ids[i]] = test_labels[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARU5RV_tPYQW"
      },
      "source": [
        "#  Gaussian kernel layer in KNRM\n",
        "def Kernel(mu, sigma):\n",
        "\n",
        "  def kernel(x):\n",
        "    return tf.math.exp(-0.5 * (x - mu) * (x - mu) / sigma / sigma)\n",
        "    \n",
        "  return Activation(kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86YIDOD_z6jG"
      },
      "source": [
        "n_filters = 50\n",
        "n_kernels = 11\n",
        "max_ngram = 3\n",
        "conv_activation = 'relu'\n",
        "\n",
        "\n",
        "#TODO use_crossmatch\n",
        "use_crossmatch= False\n",
        "\n",
        "MUs = [-0.9, -0.7, -0.5, -0.3, -0.1, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
        "SIGMAs = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.001]\n",
        "\n",
        "query = Input(name='query_input',shape=(query_max_len,))\n",
        "doc =  Input(name='doc_input',shape=(doc_maxlen,))\n",
        "\n",
        "\n",
        "q_embed = Embedding(vocab_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=query_max_len,\n",
        "                            trainable=True)(query)\n",
        "\n",
        "d_embed = Embedding(vocab_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=doc_maxlen,\n",
        "                            trainable=True)(doc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "q_convs = []\n",
        "d_convs = []\n",
        "for i in range(max_ngram):\n",
        "    c = Conv1D(\n",
        "        n_filters, i + 1,\n",
        "        activation=conv_activation,\n",
        "        padding='same'\n",
        "    )\n",
        "    q_convs.append(c(q_embed))\n",
        "    d_convs.append(c(d_embed))\n",
        "\n",
        "KM = []\n",
        "for qi in range(max_ngram):\n",
        "    for di in range(max_ngram):\n",
        "        # do not match n-gram with different length if use crossmatch\n",
        "        if not use_crossmatch and qi != di:\n",
        "            continue\n",
        "        q_ngram = q_convs[qi]\n",
        "        d_ngram = d_convs[di]\n",
        "        mm = Dot(axes=[2, 2],\n",
        "                              normalize=True)([q_ngram, d_ngram])\n",
        "\n",
        "        for i in range(n_kernels):\n",
        "            mu = MUs[i]\n",
        "            sigma = SIGMAs[i]\n",
        "            mm_exp = Kernel(mu, sigma)(mm)\n",
        "            mm_doc_sum = Lambda(\n",
        "                lambda x: tf.reduce_sum(x, 2))(\n",
        "                mm_exp)\n",
        "            mm_log = Activation(tf.math.log1p)(mm_doc_sum)\n",
        "            mm_sum = Lambda(\n",
        "                lambda x: tf.reduce_sum(x, 1))(mm_log)\n",
        "            KM.append(mm_sum)\n",
        "\n",
        "phi = Lambda(lambda x: tf.stack(x, 1))(KM)\n",
        "# out = Dense(1000, activation='relu')(phi)\n",
        "# out = Dropout(0.2)(out)\n",
        "out = Dense(1, activation='linear')(phi) # ranking\n",
        "model = Model(inputs=[query, doc], outputs=[out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9_O2wZ5AODB"
      },
      "source": [
        "def pairwise_rank_loss(y_true,y_pred):\n",
        "  pos=K.sum(y_true*y_pred,axis=-1)\n",
        "  neg=K.max((1-y_true)*y_pred,axis=-1)\n",
        "  loss = K.maximum(neg - pos + 1, 0)\n",
        "  return K.mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W0rlIjDQi5E"
      },
      "source": [
        "model.compile(loss=pairwise_rank_loss, optimizer='adam')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDv4F1yW2eCu"
      },
      "source": [
        "val_docs = []\n",
        "val_doc_ids = []\n",
        "val_query_ids = []\n",
        "val_queries = []\n",
        "val_labels = []\n",
        "errors = []\n",
        "with open('data/'+val_file,'r') as f:\n",
        "  for line in f:\n",
        "    query = line.split()[0]\n",
        "    doc = line.split()[1]\n",
        "\n",
        "    try:\n",
        "\n",
        "      label = 0.0\n",
        "      if(doc in qrels[query] and (qrels[query][doc] > 0)):\n",
        "        label = 1.0\n",
        "      training_labels.append(label)\n",
        "\n",
        "      training_doc_ids.append(doc)\n",
        "      training_query_ids.append(query)\n",
        "\n",
        "\n",
        "      training_docs.append(document_contents[doc])\n",
        "      training_queries.append(queries[query])\n",
        "\n",
        "    # get label (if in qrels and >0 then label = 1)\n",
        "\n",
        "\n",
        "    except:\n",
        "      # ignore missing \n",
        "      errors.append(query)\n",
        "\n",
        "\n",
        "tokenized_docs = tokenizer.texts_to_sequences(val_docs)\n",
        "tokenized_queries = tokenizer.texts_to_sequences(val_queries)\n",
        "\n",
        "X_docs_val = pad_sequences(tokenized_docs, maxlen=doc_maxlen)\n",
        "X_queries_val = pad_sequences(tokenized_queries, maxlen=query_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY0T_HSe7_Yt",
        "outputId": "80e51176-cb79-4cbd-ec92-03a8bea0cce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        }
      },
      "source": [
        "epochs = 10\n",
        "for i in range(epochs):\n",
        "  print(i)\n",
        "  history = model.fit([X_queries,X_docs], training_labels,\n",
        "                      epochs=1,\n",
        "                      verbose=True,\n",
        "                      # validation_data=([X_queries_test,X_docs_test], test_labels),\n",
        "                      batch_size=32)\n",
        "  \n",
        "  training_preds = model.predict([X_queries,X_docs])\n",
        "  training_preds_dicts = {}\n",
        "  for i in range(len(training_query_ids)):\n",
        "    training_preds_dicts.setdefault(training_query_ids[i], {})[training_doc_ids[i]] = training_preds[i][0]\n",
        "  scores = []\n",
        "  for qid in set(training_query_ids):\n",
        "    scores.append(ndcg_score([list(training_labels_dicts[qid].values())], [list(training_preds_dicts[qid].values())], k=20))\n",
        "  print('training ndcg@20 score = ',(sum(scores)/len(scores)))\n",
        "\n",
        "\n",
        "  test_preds = model.predict([X_queries_test,X_docs_test])\n",
        "  test_preds_dicts = {}\n",
        "  for i in range(len(test_query_ids)):\n",
        "    test_preds_dicts.setdefault(test_query_ids[i], {})[test_doc_ids[i]] = test_preds[i][0]\n",
        "\n",
        "  scores = []\n",
        "  for qid in set(test_query_ids):\n",
        "    scores.append(ndcg_score([list(test_labels_dicts[qid].values())], [list(test_preds_dicts[qid].values())], k=20))\n",
        "\n",
        "  print('test ndcg@20 score = ',sum(scores)/len(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "21914/21914 [==============================] - 124s 6ms/step - loss: 0.4064\n",
            "training ndcg@20 score =  0.911139947318693\n",
            "test ndcg@20 score =  0.31342377186425563\n",
            "1\n",
            "Epoch 1/1\n",
            "21914/21914 [==============================] - 123s 6ms/step - loss: 0.1474\n",
            "training ndcg@20 score =  0.9888974628839058\n",
            "test ndcg@20 score =  0.3389081817203834\n",
            "2\n",
            "Epoch 1/1\n",
            "21914/21914 [==============================] - 123s 6ms/step - loss: 0.0521\n",
            "training ndcg@20 score =  0.9926706093884164\n",
            "test ndcg@20 score =  0.3290152485395734\n",
            "3\n",
            "Epoch 1/1\n",
            "21914/21914 [==============================] - 123s 6ms/step - loss: 0.0249\n",
            "training ndcg@20 score =  0.9927664548125343\n",
            "test ndcg@20 score =  0.3297702324194314\n",
            "4\n",
            "Epoch 1/1\n",
            "21914/21914 [==============================] - 123s 6ms/step - loss: 0.0157\n",
            "training ndcg@20 score =  0.9930351005420304\n",
            "test ndcg@20 score =  0.3197413641103011\n",
            "5\n",
            "Epoch 1/1\n",
            " 9248/21914 [===========>..................] - ETA: 1:10 - loss: 0.0151"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-00b185cc1cbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                       \u001b[0;31m# validation_data=([X_queries_test,X_docs_test], test_labels),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                       batch_size=32)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtraining_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_queries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# For backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[1;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyBIxghs_hLG"
      },
      "source": [
        "# Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4bbOFsSHygx"
      },
      "source": [
        "# write a custom qrels file only containing the queries from the training split\n",
        "with open('qrels.robust2004.txt','r') as qrels_file:\n",
        "  with open('custom_training_qrels.txt','w') as new_file:\n",
        "    for line in qrels_file:\n",
        "      qid, _, docid, score = line.split()\n",
        "      if(qid in set(training_query_ids)):\n",
        "        new_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01KLIrX9PeKp"
      },
      "source": [
        "# write a custom qrels file only containing the queries from the test split\n",
        "with open('qrels.robust2004.txt','r') as qrels_file:\n",
        "  with open('custom_test_qrels.txt','w') as new_file:\n",
        "    for line in qrels_file:\n",
        "      qid, _, docid, score = line.split()\n",
        "      if(qid in set(test_query_ids)):\n",
        "        new_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9ijJJBkebkJ"
      },
      "source": [
        "qrels_q = []\n",
        "qrels_d = []\n",
        "qrels_to_test_doc = []\n",
        "qrels_to_test_query = []\n",
        "qdset = []\n",
        "errors = []\n",
        "with open('qrels.robust2004.txt','r') as qrels_file:\n",
        "  for line in qrels_file:\n",
        "    qid, _, docid, score = line.split()\n",
        "    if(qid in set(test_query_ids)):\n",
        "      try:\n",
        "        qrels_to_test_doc.append(document_contents[docid])\n",
        "        qrels_q.append(qid)\n",
        "        qrels_d.append(docid)\n",
        "        qrels_to_test_query.append(queries[qid])\n",
        "        qdset.append(\"%s,%s\"%(qid,docid))\n",
        "      except:\n",
        "        errors.append(docid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOKpsEPt2BsY"
      },
      "source": [
        "tokenized_docs = tokenizer.texts_to_sequences(qrels_to_test_doc)\n",
        "tokenized_queries = tokenizer.texts_to_sequences(qrels_to_test_query)\n",
        "\n",
        "X_docs_test = pad_sequences(tokenized_docs, maxlen=doc_maxlen)\n",
        "X_queries_test = pad_sequences(tokenized_queries, maxlen=query_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXJV-Isr3zYP"
      },
      "source": [
        "preds = model.predict([X_queries_test,X_docs_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRncAkye38Pe"
      },
      "source": [
        "with open('run.txt','w') as new_file:\n",
        "  for i in range(len(qrels_to_test_doc)):\n",
        "    line = \"%s %s %s %s %s %s\\n\" % (qrels_q[i],0,qrels_d[i],1,preds[i][0],\"run-name\")\n",
        "    new_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HamQV4ey0ra",
        "outputId": "88ecae09-3527-4f43-9dc1-c91cb53490a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "qrels_to_test_query"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONeWWOtHxpw3"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('drive/My Drive/errors', 'wb') as fp:\n",
        "    pickle.dump(errors, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1qZWXK9obbV",
        "outputId": "c1c1ddaa-07a0-46c8-bdca-ccefdebb3d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(errors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0npk0YFxnRLq",
        "outputId": "350d1ff6-fe45-403a-99d0-3ff0bf7ee4a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set(test_doc_ids) == set(qrels_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uev7KBLGgPLY"
      },
      "source": [
        "qdset2 = []\n",
        "\n",
        "for q,d in zip(test_query_ids,test_doc_ids):\n",
        "  qdset2.append(\"%s,%s\"%(q,d))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Ciebnhfuhj",
        "outputId": "26585c1a-ee7c-404d-c5e3-c9bf18f4ce07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(test_doc_ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7311"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7gYdP8NfWFU"
      },
      "source": [
        "test_doc_ids = []\n",
        "test_query_ids = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHSW50hDJILU"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenized_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "tokenized_queries = tokenizer.texts_to_sequences(test_queries)\n",
        "\n",
        "X_docs_test = pad_sequences(tokenized_docs, maxlen=doc_maxlen)\n",
        "X_queries_test = pad_sequences(tokenized_queries, maxlen=query_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa34CyaCLwOB"
      },
      "source": [
        "preds = model.predict([X_queries_test,X_docs_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1KkacRrNFSI",
        "outputId": "f1536782-e864-4e2d-f3de-a6bad6bf177d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds[i][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.045687955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc3lBO7W7vnf"
      },
      "source": [
        "with open('run.txt','w') as new_file:\n",
        "  for i in range(len(qrels_to_test_doc)):\n",
        "    line = \"%s %s %s %s %s %s\\n\" % (qrels_q[i],0,qrels_d[i],i,preds[i][0],\"run-name\")\n",
        "    new_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyS7ed4-RxPw"
      },
      "source": [
        "with open('run.txt','w') as new_file:\n",
        "  for i in range(len(test_docs)):\n",
        "    line = \"%s %s %s %s %s %s\\n\" % (test_query_ids[i],0,test_doc_ids[i],i,preds[i][0],\"run-name\")\n",
        "    new_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPyNM4fe8GxT",
        "outputId": "54ff1600-63a7-4042-ff18-24f25c3a4aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "!ls ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anserini\t\t\t       qrels.robust2004.txt\n",
            "custom_test_qrels.txt\t\t       qrels.robust2004.txt.1\n",
            "custom_training_qrels.txt\t       queries.tsv\n",
            "data\t\t\t\t       queries.tsv.1\n",
            "data_folds.zip\t\t\t       queries.tsv.2\n",
            "drive\t\t\t\t       run.txt\n",
            "GoogleNews-vectors-negative300.bin.gz  sample_data\n",
            "index-robust04-20191213\t\t       training_run.txt\n",
            "index-robust04-20191213.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW-wE-BEGc4f"
      },
      "source": [
        "with open('training_run.txt','w') as new_file:\n",
        "  for i in range(len(training_query_ids)):\n",
        "    line = \"%s %s %s %s %s %s\\n\" % (training_query_ids[i],0,training_doc_ids[i],1,1,\"run-name\")\n",
        "    new_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERpp5FzB8IK8"
      },
      "source": [
        "# BM25 Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-a7pKc68JPI"
      },
      "source": [
        "%%capture\n",
        "!pip install pyserini==0.8.1.0\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQKdoDX08WiJ",
        "outputId": "fd4e89ed-a9d6-4144-c679-bb7b484cab1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "!wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-robust04-20191213.tar.gz\n",
        "!tar xvfz index-robust04-20191213.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-29 14:59:11--  https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-robust04-20191213.tar.gz\n",
            "Resolving git.uwaterloo.ca (git.uwaterloo.ca)... 129.97.83.4\n",
            "Connecting to git.uwaterloo.ca (git.uwaterloo.ca)|129.97.83.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1821814915 (1.7G) [application/x-gzip]\n",
            "Saving to: ‘index-robust04-20191213.tar.gz’\n",
            "\n",
            "index-robust04-2019 100%[===================>]   1.70G  14.8MB/s    in 2m 16s  \n",
            "\n",
            "2020-05-29 15:01:38 (12.8 MB/s) - ‘index-robust04-20191213.tar.gz’ saved [1821814915/1821814915]\n",
            "\n",
            "index-robust04-20191213/\n",
            "index-robust04-20191213/_h_Lucene50_0.doc\n",
            "index-robust04-20191213/_h_Lucene50_0.tip\n",
            "index-robust04-20191213/_h_Lucene50_0.pos\n",
            "index-robust04-20191213/segments_2\n",
            "index-robust04-20191213/_h_Lucene50_0.tim\n",
            "index-robust04-20191213/_h_Lucene80_0.dvd\n",
            "index-robust04-20191213/_h.fdt\n",
            "index-robust04-20191213/_h_Lucene80_0.dvm\n",
            "index-robust04-20191213/_h.nvm\n",
            "index-robust04-20191213/_h.nvd\n",
            "index-robust04-20191213/_h.si\n",
            "index-robust04-20191213/write.lock\n",
            "index-robust04-20191213/_h.fdx\n",
            "index-robust04-20191213/_h.fnm\n",
            "index-robust04-20191213/_h.tvd\n",
            "index-robust04-20191213/_h.tvx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50017FaZ8YI-"
      },
      "source": [
        "from pyserini.search import pysearch\n",
        "from pyserini.index import pyutils\n",
        "\n",
        "searcher = pysearch.SimpleSearcher('index-robust04-20191213')\n",
        "index_utils = pyutils.IndexReaderUtils('index-robust04-20191213/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNgmdsvo8i0s"
      },
      "source": [
        "!git clone https://github.com/castorini/anserini.git\n",
        "%cd anserini\n",
        "# !mvn clean package appassembler:assemble -DskipTests -Dmaven.javadoc.skip=true\n",
        "!cd eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1WxxCCB-FRA"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "qs = []\n",
        "ds = []\n",
        "test_q = []\n",
        "test_d = []\n",
        "\n",
        "for qid in set(test_query_ids):\n",
        "  hits = searcher.search(queries[qid],500)\n",
        "  for hit in hits:\n",
        "    \n",
        "    ds.append(hit.docid)\n",
        "    qs.append(qid)\n",
        "    try:\n",
        "      content  = BeautifulSoup(index_utils.get_raw_document_contents(hit.docid), \"lxml\").text.replace('\\t', ' ').replace('\\r', ' ').replace('\\n', ' ').strip()\n",
        "      test_d.append(content)\n",
        "    except:\n",
        "      test_d.append(document_contents[hit.docid])\n",
        "    test_q.append(queries[qid])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qIlKGRE_cWg"
      },
      "source": [
        "tokenized_docs = tokenizer.texts_to_sequences(test_d)\n",
        "tokenized_queries = tokenizer.texts_to_sequences(test_q)\n",
        "\n",
        "X_docs_test = pad_sequences(tokenized_docs, maxlen=doc_maxlen)\n",
        "X_queries_test = pad_sequences(tokenized_queries, maxlen=query_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF5YSwjkAfgn"
      },
      "source": [
        "preds = model.predict([X_queries_test,X_docs_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx6FU5OWAfgp"
      },
      "source": [
        "with open('run.txt','w') as new_file:\n",
        "  for i in range(len(qs)):\n",
        "    line = \"%s %s %s %s %s %s\\n\" % (qs[i],0,ds[i],0,preds[i][0],\"run-name\")\n",
        "    new_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us6VHP9lAoCo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
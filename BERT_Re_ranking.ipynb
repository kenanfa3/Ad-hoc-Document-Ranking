{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Re-ranking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgR9AJLA4CIr"
      },
      "source": [
        "!cp \"drive/My Drive/data_folds.zip\" ./\n",
        "!unzip data_folds.zip\n",
        "\n",
        "import json\n",
        "with open('drive/My Drive/document_contents.json') as json_file:\n",
        "    document_contents = json.load(json_file)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Georgetown-IR-Lab/cedr/master/data/robust/queries.tsv\n",
        "with open('queries.tsv','r') as f:\n",
        "  queries = {}\n",
        "  for line in f:\n",
        "    cols = line.rstrip().split('\\t')\n",
        "    c_type, c_id, c_text = cols\n",
        "    queries[c_id] = c_text\n",
        "queries\n",
        "\n",
        "!wget https://trec.nist.gov/data/robust/qrels.robust2004.txt\n",
        "\n",
        "\n",
        "qrels = {}\n",
        "with open('qrels.robust2004.txt','r') as f:\n",
        "  for line in f:\n",
        "    qid, _, docid, score = line.split()\n",
        "    qrels.setdefault(qid, {})[docid] = int(score)\n",
        "\n",
        "# try one fold for now\n",
        "fold_name = \"fold_1\"\n",
        "training_file = fold_name+\".train\"\n",
        "test_file = fold_name+\".test\"\n",
        "val_file = fold_name+\".val\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHUSDZkCvGMS",
        "outputId": "f0b6b733-df51-4410-e707-8321c2cf66a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# errors here are the docs that we didn't find in the document_contents file. Those should be fetched \n",
        "\n",
        "training_docs = []\n",
        "training_doc_ids = []\n",
        "training_query_ids = []\n",
        "training_queries = []\n",
        "training_labels = []\n",
        "errors = set()\n",
        "\n",
        "with open('data/'+training_file,'r') as f:\n",
        "  for line in f:\n",
        "    query = line.split()[0]\n",
        "    doc = line.split()[1]\n",
        "    \n",
        "    try:\n",
        "      qrels[query]\n",
        "      document_contents[doc]\n",
        "      # get label (if in qrels and >0 then label = 1)\n",
        "      label = 0\n",
        "      if(doc in qrels[query] and (qrels[query][doc] > 0)):\n",
        "        label = 1\n",
        "      training_labels.append(label)\n",
        "\n",
        "      training_doc_ids.append(doc)\n",
        "      training_query_ids.append(query)\n",
        "\n",
        "\n",
        "      training_docs.append(document_contents[doc])\n",
        "      training_queries.append(queries[query])\n",
        "\n",
        "    except:\n",
        "      errors.add(doc)\n",
        "\n",
        "errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'FBIS4-32079',\n",
              " 'FBIS4-6139',\n",
              " 'FT921-15469',\n",
              " 'FT923-13931',\n",
              " 'FT923-9920',\n",
              " 'FT924-2909',\n",
              " 'FT924-5422',\n",
              " 'FT933-5923',\n",
              " 'FT941-15450',\n",
              " 'LA011689-0082',\n",
              " 'LA021490-0146',\n",
              " 'LA051690-0006',\n",
              " 'LA062889-0137',\n",
              " 'LA101790-0156'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1sv5OE64wLm"
      },
      "source": [
        "assert len(training_docs) == len(training_queries)\n",
        "assert len(training_docs) == len(training_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYPWAJRW4w9l"
      },
      "source": [
        "!pip install transformers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNcKAfkC8sfS"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import BertTokenizer, AdamW, BertForNextSentencePrediction\n",
        "\n",
        "\n",
        "LR = 0.001\n",
        "max_len = 512\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIbIrfb7L8iP"
      },
      "source": [
        "model.train()\n",
        "training_steps = int(len(training_docs)/BATCH_SIZE)+1\n",
        "losses = []\n",
        "# 1 epoch over X_train\n",
        "with tqdm(total=training_steps) as progress_bar:\n",
        "  for i in range(0, len(training_docs), BATCH_SIZE):\n",
        "    batch_q = training_queries[i:i+BATCH_SIZE]\n",
        "    batch_d = training_docs[i:i+BATCH_SIZE]\n",
        "    batch_y = torch.LongTensor(training_labels[i:i+BATCH_SIZE]).cuda()\n",
        "\n",
        "\n",
        "    encoding = tokenizer(batch_q, batch_d,padding='max_length',truncation=\"longest_first\", max_length  = max_len,return_tensors='pt')\n",
        "    input_ids = encoding['input_ids'].cuda()\n",
        "    attention_mask = encoding['attention_mask'].cuda()\n",
        "    token_type_ids = encoding['token_type_ids'].cuda()\n",
        "\n",
        "    # loss, logits = model(input_ids= input_ids,attention_mask=attention_mask, token_type_ids=token_type_ids,next_sentence_label=batch_y)\n",
        "    logits = model(input_ids= input_ids,attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
        "\n",
        "    # pairwise ranking loss\n",
        "    loss = torch.mean(1. - logits.softmax(dim=1)[:, 0])\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    avg_loss = sum(losses)/len(losses)\n",
        "    progress_bar.update(1)\n",
        "    progress_bar.set_description(\"avg loss so far = {}\".format(avg_loss))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}